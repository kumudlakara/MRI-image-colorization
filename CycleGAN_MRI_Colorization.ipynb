{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CycleGAN_MRI_Colorization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumudlakara/MRI-image-colorization/blob/main/CycleGAN_MRI_Colorization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irY2vJRElXsX"
      },
      "source": [
        "# CycleGAN for Colorization of Medical Images \n",
        "\n",
        "This code example provides a full implementation of CycleGAN in Keras. It is is based on [this implementation](https://github.com/simontomaskarlsson/CycleGAN-Keras) by Simon Karlsson. Although it is possible to run this code on a CPU, a computer with a **GPU is strongly recommended**. The code is provided ready to run, but also includes multiple adjustable settings.\n",
        "\n",
        "#### What is CycleGAN?\n",
        "CycleGAN is an unsupervised image-to-image translation architecture proposed in 2017 by [Zhu et al.](https://arxiv.org/abs/1703.10593) Its most remarkable feature is its capacity for learning mappings between classes of images without requiring paired data, making it something of a \"universal image translator\". This is achieved by complementing the regular adversarial losses seen in GANs with \"cycle consistency\" losses, which enforce $$A \\stackrel{G_{AB}}{\\rightarrow} B' \\stackrel{G_{BA}}{\\rightarrow} A'' \\approx A $$ and $$B \\stackrel{G_{BA}}{\\rightarrow} A' \\stackrel{G_{AB}}{\\rightarrow} B'' \\approx B . $$\n",
        "\n",
        "#### Directory structure:\n",
        "- **data/** contains the datasets.\n",
        "    - **data/&lt;dataset&gt;/{train_A, train_B}/** contains training images for classes A and B.\n",
        "    - **data/&lt;dataset&gt;/{test_A, test_B}/** contains testing images that are not used during training. These are useful to evaluate the generalization of the model to new data.\n",
        "- **images/** stores metadata and loss information of each CycleGAN run, as well as evaluation images.\n",
        "    - **images/meta_data.json** contains the settings of the run.\n",
        "    - **images/loss_output.csv** contains the various losses of the model, stored after every batch.\n",
        "    - **images/{train_A, train_B, test_A, test_B}** contains intermediate evaluation images for each epoch, illustrating generator performance.\n",
        "    - **images/tmp.png** shows example image translations from the current moment in training. This image updates in real time and can be used to see how the training converges.\n",
        "- **saved_models** stores the generator and discriminator models resulting from each run, which are saved every 20 epochs.\n",
        "\n",
        "#### Example data\n",
        "We provide a small example dataset with images of male and female faces. During training the CycleGAN learns to switch the genre of the faces. This dataset is small enough that the training can be run in under 15 min with a standard GPU. This allows visualization of the training progess in real time by montitoring the **images/tmp.png** file.  New datasets can be added by placing them in the **data/** folder, and can be selected by setting the `image_folder` variable below.\n",
        "\n",
        "#### Interpretation of output images\n",
        "- **images/tmp.png** has two rows. The top row shows, from left to right, the original image $A$, the translated image $B'=G_{AB}(A)$ and the recovered image $A'' = G_{BA}(B') = G_{BA}(G_{AB}(A))$. The bottom row shows similar images for the other domain: $B$, $A'$, $B''$. Here is an exmaple form the middle of training:\n",
        "\n",
        "    <img src=\"https://github.com/brainhack101/IntroDL/blob/master/notebooks/2019/Eklund/notebook_images/tmp.png?raw=1\">\n",
        "\n",
        "    The adversarial losses push the middle image in both rows to look realistic. On the other hand, the cycle consistenxy losses force the left (original) and right (reconstructed) images to be similar.\n",
        "\n",
        "- **images/{train_A, ..., test_B}** contains example results for each training epoch. If the dataset is _unpaired_ it is essentially the same as **tmp.png**:\n",
        "\n",
        "    <img src=\"https://github.com/brainhack101/IntroDL/blob/master/notebooks/2019/Eklund/notebook_images/MFepoch200.png?raw=1\">\n",
        "    \n",
        "    If the data is _paired_ a new image is added in the first position representing the ground truth for the conversion. Here is an example conversion between T1w and T2w MRI images (these images are paired because the T1w and T2w are of the same subject):\n",
        "    \n",
        "    <img src=\"https://github.com/kumudlakara/MRI-image-colorization/blob/main/mri_colourization_epoch70.png?raw=1\">\n",
        "\n",
        "    In this scenario the first image should match the third and the second should match the fourth. From left to right the imageas are $B_{GT}$, $A$, $B'$, $A''$. Note that the conversion from $A$ to $B$ matches well with the ground truth despite the fact that CycleGAN is unaware that the data is paired."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEyU05yplivE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "512fe676-0bb0-4521-ae9f-81ad70f38540"
      },
      "source": [
        "!wget -nc https://raw.githubusercontent.com/brainhack101/IntroDL/master/notebooks/2019/Eklund/requirements-gpu.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-12 09:55:46--  https://raw.githubusercontent.com/brainhack101/IntroDL/master/notebooks/2019/Eklund/requirements-gpu.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 90 [text/plain]\n",
            "Saving to: ‘requirements-gpu.txt’\n",
            "\n",
            "requirements-gpu.tx 100%[===================>]      90  --.-KB/s    in 0s      \n",
            "\n",
            "2022-03-12 09:55:47 (4.46 MB/s) - ‘requirements-gpu.txt’ saved [90/90]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 'keras==2.1.6' --force-reinstall"
      ],
      "metadata": {
        "id": "wIDJCbAHYAlw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4f2e093a-ca72-40fa-9ba6-7117a2ddc996"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras==2.1.6\n",
            "  Downloading Keras-2.1.6-py2.py3-none-any.whl (339 kB)\n",
            "\u001b[K     |████████████████████████████████| 339 kB 3.9 MB/s \n",
            "\u001b[?25hCollecting scipy>=0.14\n",
            "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 38.1 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting pyyaml\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 33.1 MB/s \n",
            "\u001b[?25hCollecting h5py\n",
            "  Downloading h5py-3.6.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 36.1 MB/s \n",
            "\u001b[?25hCollecting numpy>=1.9.1\n",
            "  Downloading numpy-1.21.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 47.0 MB/s \n",
            "\u001b[?25hCollecting six>=1.9.0\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting cached-property\n",
            "  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: numpy, cached-property, six, scipy, pyyaml, h5py, keras\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.5\n",
            "    Uninstalling numpy-1.21.5:\n",
            "      Successfully uninstalled numpy-1.21.5\n",
            "  Attempting uninstall: cached-property\n",
            "    Found existing installation: cached-property 1.5.2\n",
            "    Uninstalling cached-property-1.5.2:\n",
            "      Successfully uninstalled cached-property-1.5.2\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "tensorflow 2.8.0 requires keras<2.9,>=2.8.0rc0, but you have keras 2.1.6 which is incompatible.\n",
            "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed cached-property-1.5.2 h5py-3.6.0 keras-2.1.6 numpy-1.21.5 pyyaml-6.0 scipy-1.7.3 six-1.16.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "six"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EF-Dt-UMl8aV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f06a2ce-863b-4ab2-b8d6-69bc2a1a8753"
      },
      "source": [
        "!pip install -r requirements-gpu.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://www.github.com/keras-team/keras-contrib.git (from -r requirements-gpu.txt (line 2))\n",
            "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-nppw7nn8\n",
            "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-nppw7nn8\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from -r requirements-gpu.txt (line 1)) (2.1.6)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.7/dist-packages (from -r requirements-gpu.txt (line 3)) (1.0.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r requirements-gpu.txt (line 4)) (3.2.2)\n",
            "Collecting progress\n",
            "  Downloading progress-1.6.tar.gz (7.8 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras->-r requirements-gpu.txt (line 1)) (6.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras->-r requirements-gpu.txt (line 1)) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras->-r requirements-gpu.txt (line 1)) (1.21.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras->-r requirements-gpu.txt (line 1)) (1.16.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras->-r requirements-gpu.txt (line 1)) (3.6.0)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter->-r requirements-gpu.txt (line 3)) (4.10.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter->-r requirements-gpu.txt (line 3)) (5.2.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter->-r requirements-gpu.txt (line 3)) (5.6.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jupyter->-r requirements-gpu.txt (line 3)) (7.6.5)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter->-r requirements-gpu.txt (line 3)) (5.3.1)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter->-r requirements-gpu.txt (line 3)) (5.2.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements-gpu.txt (line 4)) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements-gpu.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements-gpu.txt (line 4)) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements-gpu.txt (line 4)) (3.0.7)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras->-r requirements-gpu.txt (line 1)) (1.5.2)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->-r requirements-gpu.txt (line 3)) (5.1.1)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->-r requirements-gpu.txt (line 3)) (5.5.0)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->-r requirements-gpu.txt (line 3)) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->-r requirements-gpu.txt (line 3)) (5.3.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->-r requirements-gpu.txt (line 3)) (57.4.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->-r requirements-gpu.txt (line 3)) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->-r requirements-gpu.txt (line 3)) (0.8.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->-r requirements-gpu.txt (line 3)) (1.0.18)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->-r requirements-gpu.txt (line 3)) (4.4.2)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->-r requirements-gpu.txt (line 3)) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->-r requirements-gpu.txt (line 3)) (2.6.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->jupyter->-r requirements-gpu.txt (line 3)) (0.2.5)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->-r requirements-gpu.txt (line 3)) (5.1.3)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->-r requirements-gpu.txt (line 3)) (3.5.2)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->-r requirements-gpu.txt (line 3)) (0.2.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->-r requirements-gpu.txt (line 3)) (1.0.2)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter->-r requirements-gpu.txt (line 3)) (4.3.3)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter->-r requirements-gpu.txt (line 3)) (4.9.2)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter->-r requirements-gpu.txt (line 3)) (5.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter->-r requirements-gpu.txt (line 3)) (4.11.2)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter->-r requirements-gpu.txt (line 3)) (0.18.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter->-r requirements-gpu.txt (line 3)) (3.10.0.2)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter->-r requirements-gpu.txt (line 3)) (21.4.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter->-r requirements-gpu.txt (line 3)) (3.7.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->-r requirements-gpu.txt (line 3)) (2.11.3)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->-r requirements-gpu.txt (line 3)) (0.13.3)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->-r requirements-gpu.txt (line 3)) (1.8.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->jupyter->-r requirements-gpu.txt (line 3)) (22.3.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter->-r requirements-gpu.txt (line 3)) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->jupyter->-r requirements-gpu.txt (line 3)) (2.0.1)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->-r requirements-gpu.txt (line 3)) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->-r requirements-gpu.txt (line 3)) (0.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->-r requirements-gpu.txt (line 3)) (4.1.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->-r requirements-gpu.txt (line 3)) (0.6.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->-r requirements-gpu.txt (line 3)) (0.8.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->-r requirements-gpu.txt (line 3)) (1.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter->-r requirements-gpu.txt (line 3)) (0.5.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter->-r requirements-gpu.txt (line 3)) (21.3)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter->-r requirements-gpu.txt (line 3)) (2.0.1)\n",
            "Building wheels for collected packages: keras-contrib, progress\n",
            "  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-py3-none-any.whl size=101077 sha256=f3297843fcd91fd178f43026266dca5f1e2a40fd92e2c68b4f4dc70398a4b7ab\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-hri_sujy/wheels/bb/1f/f2/b57495012683b6b20bbae94a3915ec79753111452d79886abc\n",
            "  Building wheel for progress (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for progress: filename=progress-1.6-py3-none-any.whl size=9632 sha256=dd49c354fba240d2f3f82c56dd6e2fa22f163b1d1fefada11c82980c197c785e\n",
            "  Stored in directory: /root/.cache/pip/wheels/8e/d7/61/498d8e27dc11e9805b01eb3539e2ee344436fc226daeb5fe87\n",
            "Successfully built keras-contrib progress\n",
            "Installing collected packages: progress, keras-contrib\n",
            "Successfully installed keras-contrib-2.0.8 progress-1.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWUFie0RlXsb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "506c0fbe-4fa4-44d7-e588-84a828b2f54e"
      },
      "source": [
        "from keras.layers import Layer, Input, Dropout, Conv2D, Activation, add, UpSampling2D, \\\n",
        "    Conv2DTranspose, Flatten\n",
        "from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization, InputSpec\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.core import Dense\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Model\n",
        "from keras.engine.topology import Container\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from progress.bar import Bar\n",
        "import datetime\n",
        "import time\n",
        "import json\n",
        "import csv\n",
        "import sys\n",
        "import os\n",
        "\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "2.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHtZSQEMV-sL",
        "outputId": "a7abedba-f584-494d-e79e-0c14ace4ae52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1l6S9SEKlXsi"
      },
      "source": [
        "Additional functions are contained in the `helper_functions.py` file. These mostly include code for loading the data and saving the resutls."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whlevvwXnWrh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31c0cd8f-ee01-42a2-802b-31e813c8a39d"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/brainhack101/IntroDL/master/notebooks/2019/Eklund/helper_funcs.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-07 17:01:10--  https://raw.githubusercontent.com/brainhack101/IntroDL/master/notebooks/2019/Eklund/helper_funcs.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13105 (13K) [text/plain]\n",
            "Saving to: ‘helper_funcs.py.2’\n",
            "\n",
            "\rhelper_funcs.py.2     0%[                    ]       0  --.-KB/s               \rhelper_funcs.py.2   100%[===================>]  12.80K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2022-03-07 17:01:11 (6.62 MB/s) - ‘helper_funcs.py.2’ saved [13105/13105]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RN0vit5qeiSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Medical-Image-Synthesis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkYEGjFHbBY0",
        "outputId": "c1fcd3b0-d090-45df-a1b8-43c54760ce46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Medical-Image-Synthesis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrnYRF9-lXsj"
      },
      "source": [
        "from helper_funcs import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6Eby_82nedN"
      },
      "source": [
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtLJxriDlXsl"
      },
      "source": [
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ushjqquylXso"
      },
      "source": [
        "#### Load data\n",
        "\n",
        "The dataset used for the run is **data/&lt;`image_folder`&gt;**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9tN-H7UlXsp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d48c14a-1e9f-4380-df7e-623cc45bc70b"
      },
      "source": [
        "image_folder = '/T1-T2'\n",
        "!pwd\n",
        "data = load_data(subfolder=image_folder)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Medical-Image-Synthesis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xomIz34dlXss"
      },
      "source": [
        "### Model parameters\n",
        "\n",
        "This CycleGAN implementation allows a lot of freedom on both the training parameters and the network architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcQJERzclXst"
      },
      "source": [
        "opt = {}\n",
        "\n",
        "# Data\n",
        "opt['channels'] =  3  #data[\"nr_of_channels\"]\n",
        "opt['img_shape'] = data[\"image_size\"] + (opt['channels'],)\n",
        "print('Image shape: ', opt['img_shape'])\n",
        "\n",
        "opt['A_train'] = data[\"trainA_images\"]\n",
        "opt['B_train'] = data[\"trainB_images\"]\n",
        "opt['A_test'] = data[\"testA_images\"]\n",
        "opt['B_test'] = data[\"testB_images\"]\n",
        "opt['testA_image_names'] = data[\"testA_image_names\"]\n",
        "opt['testB_image_names'] = data[\"testB_image_names\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ze3MSgrslXsw"
      },
      "source": [
        "CylceGAN can be used both on paired and unpaired data. The `paired_data` setting affects the presentation of output images as explained above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIXSEhU2lXsw"
      },
      "source": [
        "opt['paired_data'] = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4ga7qRdlXsy"
      },
      "source": [
        "#### Training parameters\n",
        "- `lambda_ABA` and `lambda_BAB` set the importance of the cycle consistency losses in relation to the adversarial loss `lambda_adversarial`\n",
        "- `learning_rate_D` and `learning_rate_G` are the learning rates for the discriminators and generators respectively.\n",
        "- `generator_iterations` and `discriminator_iterations` represent how many times the generators or discriminators will be trained on every batch of images. This is very useful to keep the training of both systems balanced. In this case the discriminators become successful faster than the generators, so we account for this by training the generators 3 times on every batch of images.\n",
        "- `synthetic_pool_size` sets the size of the image pool used for training the discriminators. The image pool has a certain probability of returning a synthetic image from previous iterations, thus forcing the discriminator to have a certain \"memory\". More information on this method can be found in [this paper](https://arxiv.org/abs/1612.07828).\n",
        "- `beta_1` and `beta_2` are paremeters of the [Adam](https://arxiv.org/abs/1412.6980) optimizers used on the generators and discriminators.\n",
        "- `batch_size` determines the number of images used for each update of the network weights. Due to the significant memory requirements of CycleGAN it is difficult to use a large batch size. For the small example dataset values between 1-30 may be possible.\n",
        "- `epochs` sets the number of training epochs. Each epoch goes through all the training images once. The number of epochs necessary to train a model is therefore dependent on both the number of training images available and the batch size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRB3NWjIlXsz"
      },
      "source": [
        "# Training parameters\n",
        "opt['lambda_ABA'] = 10.0  # Cyclic loss weight A_2_B\n",
        "opt['lambda_BAB'] = 10.0  # Cyclic loss weight B_2_A\n",
        "opt['lambda_adversarial'] = 1.0  # Weight for loss from discriminator guess on synthetic images\n",
        "opt['learning_rate_D'] = 2e-4\n",
        "opt['learning_rate_G'] = 2e-4\n",
        "opt['generator_iterations'] = 3  # Number of generator training iterations in each training loop\n",
        "opt['discriminator_iterations'] = 1  # Number of discriminator training iterations in each training loop\n",
        "opt['synthetic_pool_size'] = 50  # Size of image pools used for training the discriminators\n",
        "opt['beta_1'] = 0.5  # Adam parameter\n",
        "opt['beta_2'] = 0.999  # Adam parameter\n",
        "opt['batch_size'] = 10  # Number of images per batch\n",
        "opt['epochs'] = 200  # Choose multiples of 20 since the models are saved each 20th epoch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXX_5VLwlXs1"
      },
      "source": [
        "# Output parameters\n",
        "opt['save_models'] = True  # Save or not the generator and discriminator models\n",
        "opt['save_training_img'] = True  # Save or not example training results or only tmp.png\n",
        "opt['save_training_img_interval'] = 1  # Number of epoch between saves of intermediate training results\n",
        "opt['self.tmp_img_update_frequency'] = 3  # Number of batches between updates of tmp.png"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaVHf9TNlXs4"
      },
      "source": [
        "#### Architecture parameters\n",
        "- `use_instance_normalization` is supposed to allow the selection of instance normalization or batch normalization layes. At the moment only instance normalization is implemented, so this option does not do anything.\n",
        "- `use_dropout` and `use_bias` allows setting droupout layers in the generators and whether to use a bias term in the various convolutional layer in the genrators and discriminators.\n",
        "- `use_linear_decay` applies linear decay on the learning rates of the generators and discriminators,   `decay_epoch`\n",
        "- `use_patchgan` determines whether the discriminator evaluates the \"realness\" of images on a patch basis or on the whole. More information on PatchGAN can be found in [this paper](https://arxiv.org/abs/1611.07004).\n",
        "- `use_resize_convolution` provides two ways to perfrom the upsampling in the generator, with significant differences in the results. More information can be found in [this article](https://distill.pub/2016/deconv-checkerboard/). Each has its advantages, and we have managed to get successful result with both methods\n",
        "- `use_discriminator sigmoid` adds a sigmoid activation at the end of the discrimintator, forcing its output to the (0-1) range."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVnsr6pSlXs5"
      },
      "source": [
        "# Architecture parameters\n",
        "opt['use_instance_normalization'] = True  # Use instance normalization or batch normalization\n",
        "opt['use_dropout'] = False  # Dropout in residual blocks\n",
        "opt['use_bias'] = True  # Use bias\n",
        "opt['use_linear_decay'] = True  # Linear decay of learning rate, for both discriminators and generators\n",
        "opt['decay_epoch'] = 101  # The epoch where the linear decay of the learning rates start\n",
        "opt['use_patchgan'] = True  # PatchGAN - if false the discriminator learning rate should be decreased\n",
        "opt['use_resize_convolution'] = False  # Resize convolution - instead of transpose convolution in deconvolution layers (uk) - can reduce checkerboard artifacts but the blurring might affect the cycle-consistency\n",
        "opt['discriminator_sigmoid'] = True  # Add a final sigmoid activation to the discriminator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfOub3aQlXs7"
      },
      "source": [
        "# Tweaks\n",
        "opt['REAL_LABEL'] = 1.0  # Use e.g. 0.9 to avoid training the discriminators to zero loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82fBbXHYlXs9"
      },
      "source": [
        "### Model architecture\n",
        "\n",
        "#### Layer blocks\n",
        "These are the individual layer blocks that are used to build the generators and discriminator. More information can be found in the appendix of the [CycleGAN paper](https://arxiv.org/abs/1703.10593)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQuI04BwlXs-"
      },
      "source": [
        "# Discriminator layers\n",
        "def ck(model, opt, x, k, use_normalization, use_bias):\n",
        "    x = Conv2D(filters=k, kernel_size=4, strides=2, padding='same', use_bias=use_bias)(x)\n",
        "    if use_normalization:\n",
        "        x = model['normalization'](axis=3, center=True, epsilon=1e-5)(x, training=True)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    return x\n",
        "\n",
        "# First generator layer\n",
        "def c7Ak(model, opt, x, k):\n",
        "    x = Conv2D(filters=k, kernel_size=7, strides=1, padding='valid', use_bias=opt['use_bias'])(x)\n",
        "    x = model['normalization'](axis=3, center=True, epsilon=1e-5)(x, training=True)\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "# Downsampling\n",
        "def dk(model, opt, x, k):  # Should have reflection padding\n",
        "    x = Conv2D(filters=k, kernel_size=3, strides=2, padding='same', use_bias=opt['use_bias'])(x)\n",
        "    x = model['normalization'](axis=3, center=True, epsilon=1e-5)(x, training=True)\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "# Residual block\n",
        "def Rk(model, opt, x0):\n",
        "    k = int(x0.shape[-1])\n",
        "\n",
        "    # First layer\n",
        "    x = ReflectionPadding2D((1,1))(x0)\n",
        "    x = Conv2D(filters=k, kernel_size=3, strides=1, padding='valid', use_bias=opt['use_bias'])(x)\n",
        "    x = model['normalization'](axis=3, center=True, epsilon=1e-5)(x, training=True)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    if opt['use_dropout']:\n",
        "        x = Dropout(0.5)(x)\n",
        "\n",
        "    # Second layer\n",
        "    x = ReflectionPadding2D((1, 1))(x)\n",
        "    x = Conv2D(filters=k, kernel_size=3, strides=1, padding='valid', use_bias=opt['use_bias'])(x)\n",
        "    x = model['normalization'](axis=3, center=True, epsilon=1e-5)(x, training=True)\n",
        "    # Merge\n",
        "    x = add([x, x0])\n",
        "\n",
        "    return x\n",
        "\n",
        "# Upsampling\n",
        "def uk(model, opt, x, k):\n",
        "    # (up sampling followed by 1x1 convolution <=> fractional-strided 1/2)\n",
        "    if opt['use_resize_convolution']:\n",
        "        x = UpSampling2D(size=(2, 2))(x)  # Nearest neighbor upsampling\n",
        "        x = ReflectionPadding2D((1, 1))(x)\n",
        "        x = Conv2D(filters=k, kernel_size=3, strides=1, padding='valid', use_bias=opt['use_bias'])(x)\n",
        "    else:\n",
        "        x = Conv2DTranspose(filters=k, kernel_size=3, strides=2, padding='same', use_bias=opt['use_bias'])(x)  # this matches fractionally stided with stride 1/2\n",
        "    x = model['normalization'](axis=3, center=True, epsilon=1e-5)(x, training=True)\n",
        "    x = Activation('relu')(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejV66kdmlXtA"
      },
      "source": [
        "#### Architecture functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDicsY-PlXtD"
      },
      "source": [
        "def build_discriminator(model, opt, name=None):\n",
        "    # Input\n",
        "    input_img = Input(shape=opt['img_shape'])\n",
        "\n",
        "    # Layers 1-4\n",
        "    x = ck(model, opt, input_img, 64, False, True) #  Instance normalization is not used for this layer)\n",
        "    x = ck(model, opt, x, 128, True, opt['use_bias'])\n",
        "    x = ck(model, opt, x, 256, True, opt['use_bias'])\n",
        "    x = ck(model, opt, x, 512, True, opt['use_bias'])\n",
        "\n",
        "    # Layer 5: Output\n",
        "    if opt['use_patchgan']:\n",
        "        x = Conv2D(filters=1, kernel_size=4, strides=1, padding='same', use_bias=True)(x)\n",
        "    else:\n",
        "        x = Flatten()(x)\n",
        "        x = Dense(1)(x)\n",
        "\n",
        "    if opt['discriminator_sigmoid']:\n",
        "        x = Activation('sigmoid')(x)\n",
        "\n",
        "    return Model(inputs=input_img, outputs=x, name=name)\n",
        "\n",
        "def build_generator(model, opt, name=None):\n",
        "    # Layer 1: Input\n",
        "    input_img = Input(shape=opt['img_shape'])\n",
        "    x = ReflectionPadding2D((3, 3))(input_img)\n",
        "    x = c7Ak(model, opt, x, 32)\n",
        "\n",
        "    # Layer 2-3: Downsampling\n",
        "    x = dk(model, opt, x, 64)\n",
        "    x = dk(model, opt, x, 128)\n",
        "\n",
        "    # Layers 4-12: Residual blocks\n",
        "    for _ in range(4, 13):\n",
        "        x = Rk(model, opt, x)\n",
        "\n",
        "    # Layer 13:14: Upsampling\n",
        "    x = uk(model, opt, x, 64)\n",
        "    x = uk(model, opt, x, 32)\n",
        "\n",
        "    # Layer 15: Output\n",
        "    x = ReflectionPadding2D((3, 3))(x)\n",
        "    x = Conv2D(opt['channels'], kernel_size=10, strides=1, padding='valid', use_bias=True)(x)\n",
        "    #mean = dense_layer(1, activation=\"linear\", **kwargs)(x)\n",
        "    #var = dense_layer(1, activation=\"softplus\", **kwargs)(x)\n",
        "    x = Activation('tanh')(x)\n",
        "\n",
        "    return Model(inputs=input_img, outputs=x, name=name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytd1YAMXlXtF"
      },
      "source": [
        "#### Loss functions\n",
        "The discriminators use MSE loss. The generators use MSE for the adversarial losses and MAE for the cycle consistency losses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhESrpK9lXtF"
      },
      "source": [
        "# Mean squared error\n",
        "def mse(y_true, y_pred):\n",
        "    loss = tf.reduce_mean(tf.math.squared_difference(y_pred, y_true))\n",
        "    return loss\n",
        "\n",
        "# Mean absolute error\n",
        "def mae(y_true, y_pred):\n",
        "    loss = tf.reduce_mean(tf.abs(y_pred - y_true))\n",
        "    return loss\n",
        "\n",
        "\n",
        "def gaussian_nll_loss(variance_tensor, epsilon = 1e-8):\n",
        "  def nll(y_true, y_pred):\n",
        "    loss = 0.5 * K.mean(K.log(variance_tensor + epsilon) + K.square(y_true - y_pred) / (variance_tensor + epsilon))\n",
        "    return loss\n",
        "  return nll"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7B6aXKllXtI"
      },
      "source": [
        "#### Build CycleGAN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBLv_l3llXtJ"
      },
      "source": [
        "model = {}\n",
        "\n",
        "# Normalization\n",
        "model['normalization'] = InstanceNormalization\n",
        "\n",
        "# Optimizers\n",
        "model['opt_D'] = Adam(opt['learning_rate_D'], opt['beta_1'], opt['beta_2'])\n",
        "model['opt_G'] = Adam(opt['learning_rate_G'], opt['beta_1'], opt['beta_2'])\n",
        "\n",
        "# Build discriminators\n",
        "D_A = build_discriminator(model, opt, name='D_A')\n",
        "D_B = build_discriminator(model, opt, name='D_B')\n",
        "\n",
        "# Define discriminator models\n",
        "image_A = Input(shape=opt['img_shape'])\n",
        "image_B = Input(shape=opt['img_shape'])\n",
        "guess_A = D_A(image_A)\n",
        "guess_B = D_B(image_B)\n",
        "model['D_A'] = Model(inputs=image_A, outputs=guess_A, name='D_A_model')\n",
        "model['D_B'] = Model(inputs=image_B, outputs=guess_B, name='D_B_model')\n",
        "\n",
        "# Compile discriminator models\n",
        "loss_weights_D = [0.5]  # 0.5 since we train on real and synthetic images\n",
        "model['D_A'].compile(optimizer=model['opt_D'],\n",
        "                 loss=mse,\n",
        "                 loss_weights=loss_weights_D)\n",
        "model['D_B'].compile(optimizer=model['opt_D'],\n",
        "                 loss=mse,\n",
        "                 loss_weights=loss_weights_D)\n",
        "\n",
        "# Use containers to make a static copy of discriminators, used when training the generators\n",
        "model['D_A_static'] = Container(inputs=image_A, outputs=guess_A, name='D_A_static_model')\n",
        "model['D_B_static'] = Container(inputs=image_B, outputs=guess_B, name='D_B_static_model')\n",
        "\n",
        "# Do not update discriminator weights during generator training\n",
        "model['D_A_static'].trainable = False\n",
        "model['D_B_static'].trainable = False\n",
        "\n",
        "# Build generators\n",
        "model['G_A2B'] = build_generator(model, opt, name='G_A2B_model')\n",
        "model['G_B2A'] = build_generator(model, opt, name='G_B2A_model')\n",
        "\n",
        "# Define full CycleGAN model, used for training the generators\n",
        "real_A = Input(shape=opt['img_shape'], name='real_A')\n",
        "#mean_rA = Dense(1, activation=\"linear\")(real_A)\n",
        "#var_rA = Dense(1, activation=\"softplus\")(real_A)\n",
        "#print(\"mean=\", mean_rA[0])\n",
        "#print(\"var=\", var_rA[0])\n",
        "real_B = Input(shape=opt['img_shape'], name='real_B')\n",
        "synthetic_B = model['G_A2B'](real_A)\n",
        "#mean_synB = Dense(1, activation=\"linear\")(synthetic_B)\n",
        "#var_synB = Dense(1, activation=\"softplus\")(synthetic_B)\n",
        "synthetic_A = model['G_B2A'](real_B)\n",
        "#mean_synA = Dense(1, activation=\"linear\")(synthetic_A)\n",
        "#var_synA = Dense(1, activation=\"softplus\")(synthetic_A)\n",
        "dB_guess_synthetic = model['D_B_static'](synthetic_B)\n",
        "dA_guess_synthetic = model['D_A_static'](synthetic_A)\n",
        "reconstructed_A = model['G_B2A'](synthetic_B)\n",
        "mean_recA = Dense(1, activation=\"linear\")(reconstructed_A)\n",
        "var_recA = Dense(1, activation=\"softplus\")(reconstructed_A)\n",
        "reconstructed_B = model['G_A2B'](synthetic_A)\n",
        "mean_recB = Dense(1, activation=\"linear\")(reconstructed_B)\n",
        "var_recB = Dense(1, activation=\"softplus\")(reconstructed_B)\n",
        "\n",
        "# Compile full CycleGAN model\n",
        "#model_outputs = [reconstructed_A, reconstructed_B,\n",
        "#                 dB_guess_synthetic, dA_guess_synthetic]\n",
        "\n",
        "model_outputs = [mean_recA, mean_recB, dB_guess_synthetic, dA_guess_synthetic]\n",
        "\n",
        "#compile_losses = [mae, mae,\n",
        "#                  mse, mse]\n",
        "\n",
        "compile_losses = [gaussian_nll_loss(var_recA), gaussian_nll_loss(var_recB),\n",
        "                  mae, mae]\n",
        "\n",
        "compile_weights = [opt['lambda_ABA'], opt['lambda_BAB'],\n",
        "                   opt['lambda_adversarial'], opt['lambda_adversarial']]\n",
        "\n",
        "model['G_model'] = Model(inputs=[real_A, real_B],\n",
        "                     outputs=model_outputs,\n",
        "                     name='G_model')\n",
        "\n",
        "model['G_model'].compile(optimizer=model['opt_G'],\n",
        "                     loss=compile_losses,\n",
        "                     loss_weights=compile_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPgzq3VjlXtM"
      },
      "source": [
        "#### Folders and configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3yDA_O7lXtN"
      },
      "source": [
        "opt['date_time'] = time.strftime('%Y%m%d-%H%M%S', time.localtime()) + '-' + image_folder\n",
        "\n",
        "# Output folder for run data and images\n",
        "opt['out_dir'] = os.path.join('images_nll', opt['date_time'])\n",
        "if not os.path.exists(opt['out_dir']):\n",
        "    os.makedirs(opt['out_dir'])\n",
        "\n",
        "# Output folder for saved models\n",
        "if opt['save_models']:\n",
        "    opt['model_out_dir'] = os.path.join('saved_models_nll', opt['date_time'])\n",
        "    if not os.path.exists(opt['model_out_dir']):\n",
        "        os.makedirs(opt['model_out_dir'])\n",
        "\n",
        "write_metadata_to_JSON(model, opt)\n",
        "\n",
        "# Don't pre-allocate GPU memory; allocate as-needed\n",
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "#K.tensorflow_backend.set_session(tf.Session(config=config))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nk04cLeklXtQ"
      },
      "source": [
        "### Training function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Gi4VtqjlXtS"
      },
      "source": [
        "def train(model, opt):\n",
        "\n",
        "    def run_training_batch():\n",
        "\n",
        "        # ======= Discriminator training ======\n",
        "        # Generate batch of synthetic images\n",
        "        synthetic_images_B = model['G_A2B'].predict(real_images_A)\n",
        "        synthetic_images_A = model['G_B2A'].predict(real_images_B)\n",
        "        synthetic_images_B = synthetic_pool_B.query(synthetic_images_B)\n",
        "        synthetic_images_A = synthetic_pool_A.query(synthetic_images_A)\n",
        "\n",
        "        # Train discriminators on batch\n",
        "        D_loss = []\n",
        "        for _ in range(opt['discriminator_iterations']):\n",
        "            D_A_loss_real = model['D_A'].train_on_batch(x=real_images_A, y=ones)\n",
        "            D_B_loss_real = model['D_B'].train_on_batch(x=real_images_B, y=ones)\n",
        "            D_A_loss_synthetic = model['D_A'].train_on_batch(x=synthetic_images_A, y=zeros)\n",
        "            D_B_loss_synthetic = model['D_B'].train_on_batch(x=synthetic_images_B, y=zeros)\n",
        "            D_A_loss = D_A_loss_real + D_A_loss_synthetic\n",
        "            D_B_loss = D_B_loss_real + D_B_loss_synthetic\n",
        "            D_loss.append(D_A_loss + D_B_loss)\n",
        "\n",
        "        # ======= Generator training ==========\n",
        "        target_data = [real_images_A, real_images_B, ones, ones]  # Reconstructed images need to match originals, discriminators need to predict ones\n",
        "\n",
        "        # Train generators on batch\n",
        "        G_loss = []\n",
        "        for _ in range(opt['generator_iterations']):\n",
        "            G_loss.append(model['G_model'].train_on_batch(\n",
        "                x=[real_images_A, real_images_B], y=target_data))\n",
        "\n",
        "        # =====================================\n",
        "\n",
        "        # Update learning rates\n",
        "        if opt['use_linear_decay'] and epoch >= opt['decay_epoch']:\n",
        "            update_lr(model['D_A'], decay_D)\n",
        "            update_lr(model['D_B'], decay_D)\n",
        "            update_lr(model['G_model'], decay_G)\n",
        "\n",
        "        # Store training losses\n",
        "        D_A_losses.append(D_A_loss)\n",
        "        D_B_losses.append(D_B_loss)\n",
        "        D_losses.append(D_loss[-1])\n",
        "\n",
        "        ABA_reconstruction_loss = G_loss[-1][1]\n",
        "        BAB_reconstruction_loss = G_loss[-1][2]\n",
        "        reconstruction_loss = ABA_reconstruction_loss + BAB_reconstruction_loss\n",
        "        G_AB_adversarial_loss = G_loss[-1][3]\n",
        "        G_BA_adversarial_loss = G_loss[-1][4]\n",
        "\n",
        "        ABA_reconstruction_losses.append(ABA_reconstruction_loss)\n",
        "        BAB_reconstruction_losses.append(BAB_reconstruction_loss)\n",
        "        reconstruction_losses.append(reconstruction_loss)\n",
        "        G_AB_adversarial_losses.append(G_AB_adversarial_loss)\n",
        "        G_BA_adversarial_losses.append(G_BA_adversarial_loss)\n",
        "        G_losses.append(G_loss[-1][0])\n",
        "\n",
        "        # Print training status\n",
        "        print('\\n')\n",
        "        print('Epoch ---------------------', epoch, '/', opt['epochs'])\n",
        "        print('Loop index ----------------', loop_index + 1, '/', nr_im_per_epoch)\n",
        "        if opt['discriminator_iterations'] > 1:\n",
        "            print('  Discriminator losses:')\n",
        "            for i in range(opt['discriminator_iterations']):\n",
        "                print('D_loss', D_loss[i])\n",
        "        if opt['generator_iterations'] > 1:\n",
        "            print('  Generator losses:')\n",
        "            for i in range(opt['generator_iterations']):\n",
        "                print('G_loss', G_loss[i])\n",
        "        print('  Summary:')\n",
        "        print('D_lr:', K.get_value(model['D_A'].optimizer.lr))\n",
        "        print('G_lr', K.get_value(model['G_model'].optimizer.lr))\n",
        "        print('D_loss: ', D_loss[-1])\n",
        "        print('G_loss: ', G_loss[-1][0])\n",
        "        print('reconstruction_loss: ', reconstruction_loss)\n",
        "        print_ETA(opt, start_time, epoch, nr_im_per_epoch, loop_index)\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        if loop_index % 3*opt['batch_size'] == 0:\n",
        "            # Save temporary images continously\n",
        "            save_tmp_images(model, opt, real_images_A[0], real_images_B[0],\n",
        "                                 synthetic_images_A[0], synthetic_images_B[0])\n",
        "\n",
        "    # ======================================================================\n",
        "    # Begin training\n",
        "    # ======================================================================\n",
        "    if opt['save_training_img'] and not os.path.exists(os.path.join(opt['out_dir'], 'train_A')):\n",
        "        os.makedirs(os.path.join(opt['out_dir'], 'train_A'))\n",
        "        os.makedirs(os.path.join(opt['out_dir'], 'train_B'))\n",
        "        os.makedirs(os.path.join(opt['out_dir'], 'test_A'))\n",
        "        os.makedirs(os.path.join(opt['out_dir'], 'test_B'))\n",
        "\n",
        "    D_A_losses = []\n",
        "    D_B_losses = []\n",
        "    D_losses = []\n",
        "\n",
        "    ABA_reconstruction_losses = []\n",
        "    BAB_reconstruction_losses = []\n",
        "    reconstruction_losses = []\n",
        "    G_AB_adversarial_losses = []\n",
        "    G_BA_adversarial_losses = []\n",
        "    G_losses = []\n",
        "\n",
        "    # Image pools used to update the discriminators\n",
        "    synthetic_pool_A = ImagePool(opt['synthetic_pool_size'])\n",
        "    synthetic_pool_B = ImagePool(opt['synthetic_pool_size'])\n",
        "\n",
        "    # Labels used for discriminator training\n",
        "    label_shape = (opt['batch_size'],) + model['D_A'].output_shape[1:]\n",
        "    ones = np.ones(shape=label_shape) * opt['REAL_LABEL']\n",
        "    zeros = ones * 0\n",
        "\n",
        "    # Linear learning rate decay\n",
        "    if opt['use_linear_decay']:\n",
        "        decay_D, decay_G = get_lr_linear_decay_rate(opt)\n",
        "\n",
        "    nr_train_im_A = opt['A_train'].shape[0]\n",
        "    nr_train_im_B = opt['B_train'].shape[0]\n",
        "    nr_im_per_epoch = int(np.ceil(np.max((nr_train_im_A, nr_train_im_B)) / opt['batch_size']) * opt['batch_size'])\n",
        "\n",
        "    # Start stopwatch for ETAs\n",
        "    start_time = time.time()\n",
        "    timer_started = False\n",
        "\n",
        "    for epoch in range(1, opt['epochs'] + 1):\n",
        "        # random_order_A = np.random.randint(nr_train_im_A, size=nr_im_per_epoch)\n",
        "        # random_order_B = np.random.randint(nr_train_im_B, size=nr_im_per_epoch)\n",
        "\n",
        "        random_order_A = np.concatenate((np.random.permutation(nr_train_im_A),\n",
        "                                         np.random.randint(nr_train_im_A, size=nr_im_per_epoch - nr_train_im_A)))\n",
        "        random_order_B = np.concatenate((np.random.permutation(nr_train_im_B),\n",
        "                                         np.random.randint(nr_train_im_B, size=nr_im_per_epoch - nr_train_im_B)))\n",
        "\n",
        "        # Train on image batch\n",
        "        for loop_index in range(0, nr_im_per_epoch, opt['batch_size']):\n",
        "            indices_A = random_order_A[loop_index:loop_index + opt['batch_size']]\n",
        "            indices_B = random_order_B[loop_index:loop_index + opt['batch_size']]\n",
        "\n",
        "            real_images_A = opt['A_train'][indices_A]\n",
        "            real_images_B = opt['B_train'][indices_B]\n",
        "\n",
        "            # Train on image batch\n",
        "            run_training_batch()\n",
        "\n",
        "            # Start timer after first (slow) iteration has finished\n",
        "            if not timer_started:\n",
        "                start_time = time.time()\n",
        "                timer_started = True\n",
        "\n",
        "        # Save training images\n",
        "        if opt['save_training_img'] and epoch % opt['save_training_img_interval'] == 0:\n",
        "            print('\\n', '\\n', '-------------------------Saving images for epoch', epoch, '-------------------------', '\\n', '\\n')\n",
        "            save_epoch_images(model, opt, epoch)\n",
        "\n",
        "        # Save model\n",
        "        if opt['save_models'] and epoch % 20 == 0:\n",
        "            save_model(opt, model['D_A'], epoch)\n",
        "            save_model(opt, model['D_B'], epoch)\n",
        "            save_model(opt, model['G_A2B'], epoch)\n",
        "            save_model(opt, model['G_B2A'], epoch)\n",
        "\n",
        "        # Save training history\n",
        "        training_history = {\n",
        "            'DA_losses': D_A_losses,\n",
        "            'DB_losses': D_B_losses,\n",
        "            'G_AB_adversarial_losses': G_AB_adversarial_losses,\n",
        "            'G_BA_adversarial_losses': G_BA_adversarial_losses,\n",
        "            'ABA_reconstruction_losses': ABA_reconstruction_losses,\n",
        "            'BAB_reconstruction_losses': BAB_reconstruction_losses,\n",
        "            'reconstruction_losses': reconstruction_losses,\n",
        "            'D_losses': D_losses,\n",
        "            'G_losses': G_losses}\n",
        "        #write_loss_data_to_file(opt, training_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7G-fVM6flXtW"
      },
      "source": [
        "### Train CycleGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_0XjcQ3lXtW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "31e58dfd-a773-4cfe-c409-7b259bed065f"
      },
      "source": [
        "train(model, opt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Epoch --------------------- 1 / 200\n",
            "Loop index ---------------- 1 / 40\n",
            "  Generator losses:\n",
            "G_loss [3.2304015, 0.0016045934, 0.16882165, 0.7413062, 0.7848329]\n",
            "G_loss [2.6161237, -0.01277999, 0.1305457, 0.6632326, 0.7752336]\n",
            "G_loss [2.0817218, -0.023834288, 0.094133615, 0.6166929, 0.7620356]\n",
            "  Summary:\n",
            "D_lr: 0.0002\n",
            "G_lr 0.0002\n",
            "D_loss:  0.43543965\n",
            "G_loss:  2.0817218\n",
            "reconstruction_loss:  0.07029933\n",
            "Elapsed time 0:00:10 : ETA in 8800 days, 5:33:24\n",
            "\n",
            "\n",
            "Epoch --------------------- 1 / 200\n",
            "Loop index ---------------- 11 / 40\n",
            "  Generator losses:\n",
            "G_loss [2.1237607, -0.025911296, 0.07836653, 0.76577204, 0.83343613]\n",
            "G_loss [1.8494539, -0.0318777, 0.064646445, 0.71554047, 0.8062261]\n",
            "G_loss [1.5317142, -0.033178665, 0.040272888, 0.65206075, 0.8087112]\n",
            "  Summary:\n",
            "D_lr: 0.0002\n",
            "G_lr 0.0002\n",
            "D_loss:  0.37515587\n",
            "G_loss:  1.5317142\n",
            "reconstruction_loss:  0.007094223\n",
            "Elapsed time 0:00:10 : ETA in 2:06:58\n",
            "\n",
            "\n",
            "Epoch --------------------- 1 / 200\n",
            "Loop index ---------------- 21 / 40\n",
            "  Generator losses:\n",
            "G_loss [1.5445373, -0.04040735, 0.029908607, 0.78451526, 0.8650094]\n",
            "G_loss [1.3666482, -0.042749465, 0.022237813, 0.72514266, 0.846622]\n",
            "G_loss [1.1725525, -0.04330081, 0.014264101, 0.64191765, 0.8210019]\n",
            "  Summary:\n",
            "D_lr: 0.0002\n",
            "G_lr 0.0002\n",
            "D_loss:  0.3004975\n",
            "G_loss:  1.1725525\n",
            "reconstruction_loss:  -0.02903671\n",
            "Elapsed time 0:00:19 : ETA in 2:06:40\n",
            "\n",
            "\n",
            "Epoch --------------------- 1 / 200\n",
            "Loop index ---------------- 31 / 40\n",
            "  Generator losses:\n",
            "G_loss [1.3390307, -0.045428637, 0.013100183, 0.78889763, 0.8734177]\n",
            "G_loss [1.2187366, -0.047678433, 0.007561206, 0.75896364, 0.8609452]\n",
            "G_loss [1.1256075, -0.04845041, 0.0043853046, 0.72139186, 0.84486675]\n",
            "  Summary:\n",
            "D_lr: 0.0002\n",
            "G_lr 0.0002\n",
            "D_loss:  0.23058403\n",
            "G_loss:  1.1256075\n",
            "reconstruction_loss:  -0.044065107\n",
            "Elapsed time 0:00:29 : ETA in 2:06:35\n",
            "\n",
            " \n",
            " -------------------------Saving images for epoch 1 ------------------------- \n",
            " \n",
            "\n",
            "\n",
            "\n",
            "Epoch --------------------- 2 / 200\n",
            "Loop index ---------------- 1 / 40\n",
            "  Generator losses:\n",
            "G_loss [1.307862, -0.050321884, 0.00817853, 0.8174567, 0.9118389]\n",
            "G_loss [1.3303597, -0.04856124, 0.013730797, 0.78611153, 0.89255255]\n",
            "G_loss [1.1681389, -0.051528536, 0.0047282754, 0.76163054, 0.87451094]\n",
            "  Summary:\n",
            "D_lr: 0.0002\n",
            "G_lr 0.0002\n",
            "D_loss:  0.18852384\n",
            "G_loss:  1.1681389\n",
            "reconstruction_loss:  -0.04680026\n",
            "Elapsed time 0:00:39 : ETA in 2:08:50\n",
            "\n",
            "\n",
            "Epoch --------------------- 2 / 200\n",
            "Loop index ---------------- 11 / 40\n",
            "  Generator losses:\n",
            "G_loss [1.2899002, -0.048614554, 0.00040229698, 0.84043235, 0.93159044]\n",
            "G_loss [1.2082772, -0.05142044, -0.0030477697, 0.82874984, 0.9242095]\n",
            "G_loss [1.1402073, -0.054063983, -0.0046001254, 0.8136325, 0.91321594]\n",
            "  Summary:\n",
            "D_lr: 0.0002\n",
            "G_lr 0.0002\n",
            "D_loss:  0.14067653\n",
            "G_loss:  1.1402073\n",
            "reconstruction_loss:  -0.05866411\n",
            "Elapsed time 0:00:49 : ETA in 2:08:39\n",
            "\n",
            "\n",
            "Epoch --------------------- 2 / 200\n",
            "Loop index ---------------- 21 / 40\n",
            "  Generator losses:\n",
            "G_loss [1.1418322, -0.05690705, -0.0056111286, 0.8265889, 0.9404251]\n",
            "G_loss [0.9904392, -0.05805678, -0.010500843, 0.74007577, 0.9359396]\n",
            "G_loss [0.84983855, -0.051239826, -0.013639508, 0.57365066, 0.92498124]\n",
            "  Summary:\n",
            "D_lr: 0.0002\n",
            "G_lr 0.0002\n",
            "D_loss:  0.15870897\n",
            "G_loss:  0.84983855\n",
            "reconstruction_loss:  -0.064879335\n",
            "Elapsed time 0:00:58 : ETA in 2:08:00\n",
            "\n",
            "\n",
            "Epoch --------------------- 2 / 200\n",
            "Loop index ---------------- 31 / 40\n",
            "  Generator losses:\n",
            "G_loss [1.5274051, -0.050187968, 0.02386424, 0.8656643, 0.92497814]\n",
            "G_loss [1.8741119, -0.049741264, 0.06142869, 0.86988723, 0.8873504]\n",
            "G_loss [1.7334661, -0.0279115, 0.021357909, 0.8717596, 0.92724246]\n",
            "  Summary:\n",
            "D_lr: 0.0002\n",
            "G_lr 0.0002\n",
            "D_loss:  0.18411234\n",
            "G_loss:  1.7334661\n",
            "reconstruction_loss:  -0.0065535903\n",
            "Elapsed time 0:01:08 : ETA in 2:07:30\n",
            "\n",
            " \n",
            " -------------------------Saving images for epoch 2 ------------------------- \n",
            " \n",
            "\n",
            "\n",
            "\n",
            "Epoch --------------------- 3 / 200\n",
            "Loop index ---------------- 1 / 40\n",
            "  Generator losses:\n",
            "G_loss [1.476965, -0.044309836, 0.006871289, 0.9002145, 0.951136]\n",
            "G_loss [1.337739, -0.049090445, -0.0017954629, 0.8917219, 0.9548761]\n",
            "G_loss [1.2069697, -0.056604944, -0.0070916964, 0.8901267, 0.95380944]\n",
            "  Summary:\n",
            "D_lr: 0.0002\n",
            "G_lr 0.0002\n",
            "D_loss:  0.08613442\n",
            "G_loss:  1.2069697\n",
            "reconstruction_loss:  -0.06369664\n",
            "Elapsed time 0:01:18 : ETA in 2:08:20\n",
            "\n",
            "\n",
            "Epoch --------------------- 3 / 200\n",
            "Loop index ---------------- 11 / 40\n",
            "  Generator losses:\n",
            "G_loss [1.2092092, -0.057412826, -0.007905903, 0.90120584, 0.9611907]\n",
            "G_loss [1.1222728, -0.060052525, -0.012064037, 0.88463324, 0.95880526]\n",
            "G_loss [1.0347725, -0.06238047, -0.015278889, 0.854709, 0.95665705]\n",
            "  Summary:\n",
            "D_lr: 0.0002\n",
            "G_lr 0.0002\n",
            "D_loss:  0.043898344\n",
            "G_loss:  1.0347725\n",
            "reconstruction_loss:  -0.07765936\n",
            "Elapsed time 0:01:27 : ETA in 2:08:05\n",
            "\n",
            "\n",
            "Epoch --------------------- 3 / 200\n",
            "Loop index ---------------- 21 / 40\n",
            "  Generator losses:\n",
            "G_loss [1.0231525, -0.06605698, -0.01844472, 0.9055468, 0.96262264]\n",
            "G_loss [0.97366685, -0.06786321, -0.020167679, 0.8938898, 0.960086]\n",
            "G_loss [0.9295595, -0.06923821, -0.02149155, 0.88001806, 0.9568391]\n",
            "  Summary:\n",
            "D_lr: 0.0002\n",
            "G_lr 0.0002\n",
            "D_loss:  0.04744624\n",
            "G_loss:  0.9295595\n",
            "reconstruction_loss:  -0.09072976\n",
            "Elapsed time 0:01:37 : ETA in 2:07:37\n",
            "\n",
            "\n",
            "Epoch --------------------- 3 / 200\n",
            "Loop index ---------------- 31 / 40\n",
            "  Generator losses:\n",
            "G_loss [0.92107034, -0.071157426, -0.024532957, 0.9170017, 0.9609724]\n",
            "G_loss [0.8731487, -0.07262495, -0.026826829, 0.9092835, 0.95838296]\n",
            "G_loss [0.8385913, -0.07354457, -0.028181363, 0.899706, 0.95614463]\n",
            "  Summary:\n",
            "D_lr: 0.0002\n",
            "G_lr 0.0002\n",
            "D_loss:  0.02887354\n",
            "G_loss:  0.8385913\n",
            "reconstruction_loss:  -0.101725936\n",
            "Elapsed time 0:01:46 : ETA in 2:07:11\n",
            "\n",
            " \n",
            " -------------------------Saving images for epoch 3 ------------------------- \n",
            " \n",
            "\n",
            "\n",
            "\n",
            "Epoch --------------------- 4 / 200\n",
            "Loop index ---------------- 1 / 40\n",
            "  Generator losses:\n",
            "G_loss [0.9285687, -0.072282575, -0.021746146, 0.90745413, 0.9614018]\n",
            "G_loss [0.8973659, -0.07283257, -0.023238288, 0.89806986, 0.9600047]\n",
            "G_loss [0.85472906, -0.07389293, -0.025105443, 0.88659203, 0.9581207]\n",
            "  Summary:\n",
            "D_lr: 0.0002\n",
            "G_lr 0.0002\n",
            "D_loss:  0.029519305\n",
            "G_loss:  0.85472906\n",
            "reconstruction_loss:  -0.09899837\n",
            "Elapsed time 0:01:57 : ETA in 2:07:36\n",
            "\n",
            "\n",
            "Epoch --------------------- 4 / 200\n",
            "Loop index ---------------- 11 / 40\n",
            "  Generator losses:\n",
            "G_loss [0.84448427, -0.07243365, -0.031222766, 0.9175394, 0.96350896]\n",
            "G_loss [0.8156598, -0.073578626, -0.032261718, 0.911928, 0.9621353]\n",
            "G_loss [0.7798953, -0.073979676, -0.03381748, 0.8970995, 0.9607674]\n",
            "  Summary:\n",
            "D_lr: 0.0002\n",
            "G_lr 0.0002\n",
            "D_loss:  0.030165788\n",
            "G_loss:  0.7798953\n",
            "reconstruction_loss:  -0.10779716\n",
            "Elapsed time 0:02:06 : ETA in 2:07:23\n",
            "\n",
            "\n",
            "Epoch --------------------- 4 / 200\n",
            "Loop index ---------------- 21 / 40\n",
            "  Generator losses:\n",
            "G_loss [0.7664525, -0.07572679, -0.031863745, 0.87575734, 0.9666006]\n",
            "G_loss [0.72924733, -0.07605704, -0.03255994, 0.8501321, 0.96528494]\n",
            "G_loss [0.6794276, -0.07664643, -0.033850893, 0.82008374, 0.9643172]\n",
            "  Summary:\n",
            "D_lr: 0.0002\n",
            "G_lr 0.0002\n",
            "D_loss:  0.048132185\n",
            "G_loss:  0.6794276\n",
            "reconstruction_loss:  -0.110497326\n",
            "Elapsed time 0:02:16 : ETA in 2:06:58\n",
            "\n",
            "\n",
            "Epoch --------------------- 4 / 200\n",
            "Loop index ---------------- 31 / 40\n",
            "  Generator losses:\n",
            "G_loss [0.8083823, -0.07563256, -0.033559624, 0.93405974, 0.96624434]\n",
            "G_loss [0.82426465, -0.07615061, -0.03084673, 0.93042946, 0.96380854]\n",
            "G_loss [0.9097233, -0.07059957, -0.027077755, 0.92349607, 0.9630005]\n",
            "  Summary:\n",
            "D_lr: 0.0002\n",
            "G_lr 0.0002\n",
            "D_loss:  0.1599217\n",
            "G_loss:  0.9097233\n",
            "reconstruction_loss:  -0.09767733\n",
            "Elapsed time 0:02:25 : ETA in 2:06:35\n",
            "\n",
            " \n",
            " -------------------------Saving images for epoch 4 ------------------------- \n",
            " \n",
            "\n",
            "\n",
            "\n",
            "Epoch --------------------- 5 / 200\n",
            "Loop index ---------------- 1 / 40\n",
            "  Generator losses:\n",
            "G_loss [0.7945802, -0.078428425, -0.028405597, 0.8953161, 0.9676043]\n",
            "G_loss [0.696567, -0.079778135, -0.03373849, 0.8634202, 0.9683131]\n",
            "G_loss [0.61123556, -0.0786896, -0.03415928, 0.77143466, 0.9682897]\n",
            "  Summary:\n",
            "D_lr: 0.0002\n",
            "G_lr 0.0002\n",
            "D_loss:  0.17345743\n",
            "G_loss:  0.61123556\n",
            "reconstruction_loss:  -0.11284888\n",
            "Elapsed time 0:02:35 : ETA in 2:06:49\n",
            "\n",
            "\n",
            "Epoch --------------------- 5 / 200\n",
            "Loop index ---------------- 11 / 40\n",
            "  Generator losses:\n",
            "G_loss [0.7741437, -0.07704987, -0.033268616, 0.90841323, 0.9689153]\n",
            "G_loss [0.7333356, -0.0782081, -0.034674916, 0.8942088, 0.9679569]\n",
            "G_loss [0.667761, -0.079299636, -0.037352655, 0.8682081, 0.9660758]\n",
            "  Summary:\n",
            "D_lr: 0.0002\n",
            "G_lr 0.0002\n",
            "D_loss:  0.16266453\n",
            "G_loss:  0.667761\n",
            "reconstruction_loss:  -0.116652295\n",
            "Elapsed time 0:02:45 : ETA in 2:06:35\n",
            "\n",
            "\n",
            "Epoch --------------------- 5 / 200\n",
            "Loop index ---------------- 21 / 40\n",
            "  Generator losses:\n",
            "G_loss [0.79490167, -0.076038055, -0.031114062, 0.8965521, 0.96987075]\n",
            "G_loss [0.7428304, -0.0774007, -0.032970145, 0.87675154, 0.96978724]\n",
            "G_loss [0.69870096, -0.07692258, -0.03277636, 0.82745284, 0.9682375]\n",
            "  Summary:\n",
            "D_lr: 0.0002\n",
            "G_lr 0.0002\n",
            "D_loss:  0.13272567\n",
            "G_loss:  0.69870096\n",
            "reconstruction_loss:  -0.10969894\n",
            "Elapsed time 0:02:54 : ETA in 2:06:15\n",
            "\n",
            "\n",
            "Epoch --------------------- 5 / 200\n",
            "Loop index ---------------- 31 / 40\n",
            "  Generator losses:\n",
            "G_loss [0.71540123, -0.079051904, -0.029791836, 0.8311551, 0.97268355]\n",
            "G_loss [1.056456, -0.060893457, -0.0026481203, 0.7202163, 0.9716555]\n",
            "G_loss [1.8476675, -0.03596349, 0.03136859, 0.92209756, 0.9715188]\n",
            "  Summary:\n",
            "D_lr: 0.0002\n",
            "G_lr 0.0002\n",
            "D_loss:  0.18269427\n",
            "G_loss:  1.8476675\n",
            "reconstruction_loss:  -0.0045948997\n",
            "Elapsed time 0:03:04 : ETA in 2:05:56\n",
            "\n",
            " \n",
            " -------------------------Saving images for epoch 5 ------------------------- \n",
            " \n",
            "\n",
            "\n",
            "\n",
            "Epoch --------------------- 6 / 200\n",
            "Loop index ---------------- 1 / 40\n",
            "  Generator losses:\n",
            "G_loss [1.290968, -0.060833387, 0.001135035, 0.9137726, 0.9741789]\n",
            "G_loss [1.1702864, -0.06551885, -0.0070963195, 0.92103356, 0.97540456]\n",
            "G_loss [0.9986159, -0.072901994, -0.01502762, 0.8996502, 0.9782618]\n",
            "  Summary:\n",
            "D_lr: 0.0002\n",
            "G_lr 0.0002\n",
            "D_loss:  0.10450528\n",
            "G_loss:  0.9986159\n",
            "reconstruction_loss:  -0.087929614\n",
            "Elapsed time 0:03:14 : ETA in 2:06:07\n",
            "\n",
            "\n",
            "Epoch --------------------- 6 / 200\n",
            "Loop index ---------------- 11 / 40\n",
            "  Generator losses:\n",
            "G_loss [0.8512915, -0.07745972, -0.029211277, 0.9395257, 0.97847575]\n",
            "G_loss [0.76845956, -0.07976312, -0.03323272, 0.9208635, 0.97755444]\n",
            "G_loss [0.67439395, -0.08099881, -0.037049573, 0.87823266, 0.9766451]\n",
            "  Summary:\n",
            "D_lr: 0.0002\n",
            "G_lr 0.0002\n",
            "D_loss:  0.03838132\n",
            "G_loss:  0.67439395\n",
            "reconstruction_loss:  -0.118048385\n",
            "Elapsed time 0:03:24 : ETA in 2:05:56\n",
            "\n",
            "\n",
            "Epoch --------------------- 6 / 200\n",
            "Loop index ---------------- 21 / 40\n",
            "  Generator losses:\n",
            "G_loss [0.71555156, -0.08144711, -0.038167603, 0.9350964, 0.9766024]\n",
            "G_loss [0.66492915, -0.0825408, -0.04019606, 0.91697806, 0.9753197]\n",
            "G_loss [0.61222243, -0.08328773, -0.042299967, 0.894045, 0.97405446]\n",
            "  Summary:\n",
            "D_lr: 0.0002\n",
            "G_lr 0.0002\n",
            "D_loss:  0.030660788\n",
            "G_loss:  0.61222243\n",
            "reconstruction_loss:  -0.1255877\n",
            "Elapsed time 0:03:33 : ETA in 2:05:39\n",
            "\n",
            "\n",
            "Epoch --------------------- 6 / 200\n",
            "Loop index ---------------- 31 / 40\n",
            "  Generator losses:\n",
            "G_loss [0.72752494, -0.0804439, -0.03865074, 0.9432519, 0.97521937]\n",
            "G_loss [0.6919493, -0.081471, -0.040304795, 0.9356001, 0.97410715]\n",
            "G_loss [0.6593612, -0.08209299, -0.041864514, 0.9259922, 0.97294414]\n",
            "  Summary:\n",
            "D_lr: 0.0002\n",
            "G_lr 0.0002\n",
            "D_loss:  0.059366442\n",
            "G_loss:  0.6593612\n",
            "reconstruction_loss:  -0.12395751\n",
            "Elapsed time 0:03:43 : ETA in 2:05:21\n",
            "\n",
            " \n",
            " -------------------------Saving images for epoch 6 ------------------------- \n",
            " \n",
            "\n",
            "\n",
            "\n",
            "Epoch --------------------- 7 / 200\n",
            "Loop index ---------------- 1 / 40\n",
            "  Generator losses:\n",
            "G_loss [0.5925679, -0.083314635, -0.043938085, 0.8910359, 0.9740592]\n",
            "G_loss [0.5542356, -0.083941065, -0.044964887, 0.87011874, 0.97317636]\n",
            "G_loss [0.5069907, -0.08392537, -0.04534043, 0.8279454, 0.97170323]\n",
            "  Summary:\n",
            "D_lr: 0.0002\n",
            "G_lr 0.0002\n",
            "D_loss:  0.118110076\n",
            "G_loss:  0.5069907\n",
            "reconstruction_loss:  -0.1292658\n",
            "Elapsed time 0:03:53 : ETA in 2:05:27\n",
            "\n",
            "\n",
            "Epoch --------------------- 7 / 200\n",
            "Loop index ---------------- 11 / 40\n",
            "  Generator losses:\n",
            "G_loss [0.6036841, -0.08361572, -0.046729613, 0.93495387, 0.9721836]\n",
            "G_loss [0.5895157, -0.08377899, -0.047766723, 0.93279517, 0.9721776]\n",
            "G_loss [0.55088264, -0.08549747, -0.04887444, 0.9238025, 0.97079927]\n",
            "  Summary:\n",
            "D_lr: 0.0002\n",
            "G_lr 0.0002\n",
            "D_loss:  0.07896829\n",
            "G_loss:  0.55088264\n",
            "reconstruction_loss:  -0.1343719\n",
            "Elapsed time 0:04:02 : ETA in 2:05:15\n",
            "\n",
            "\n",
            "Epoch --------------------- 7 / 200\n",
            "Loop index ---------------- 21 / 40\n",
            "  Generator losses:\n",
            "G_loss [0.53509945, -0.086542904, -0.04720809, 0.89954376, 0.97306556]\n",
            "G_loss [0.5004829, -0.086067215, -0.04697753, 0.85836136, 0.972569]\n",
            "G_loss [0.4069373, -0.0855935, -0.048519757, 0.7745182, 0.97355163]\n",
            "  Summary:\n",
            "D_lr: 0.0002\n",
            "G_lr 0.0002\n",
            "D_loss:  0.079037115\n",
            "G_loss:  0.4069373\n",
            "reconstruction_loss:  -0.13411325\n",
            "Elapsed time 0:04:12 : ETA in 2:05:00\n",
            "\n",
            "\n",
            "Epoch --------------------- 7 / 200\n",
            "Loop index ---------------- 31 / 40\n",
            "  Generator losses:\n",
            "G_loss [0.74118084, -0.07477828, -0.042601082, 0.9401601, 0.9748144]\n",
            "G_loss [0.8738604, -0.07176394, -0.035713486, 0.97229785, 0.97633684]\n",
            "G_loss [0.7414274, -0.08070561, -0.039932534, 0.97203094, 0.9757779]\n",
            "  Summary:\n",
            "D_lr: 0.0002\n",
            "G_lr 0.0002\n",
            "D_loss:  0.14417644\n",
            "G_loss:  0.7414274\n",
            "reconstruction_loss:  -0.12063815\n",
            "Elapsed time 0:04:21 : ETA in 2:04:43\n",
            "\n",
            " \n",
            " -------------------------Saving images for epoch 7 ------------------------- \n",
            " \n",
            "\n",
            "\n",
            "\n",
            "Epoch --------------------- 8 / 200\n",
            "Loop index ---------------- 1 / 40\n",
            "  Generator losses:\n",
            "G_loss [0.5986891, -0.08445715, -0.044578563, 0.91371524, 0.97533095]\n",
            "G_loss [0.5472252, -0.08598161, -0.046737883, 0.89980394, 0.97461617]\n",
            "G_loss [0.49226004, -0.08733434, -0.048487537, 0.87623566, 0.97424316]\n",
            "  Summary:\n",
            "D_lr: 0.0002\n",
            "G_lr 0.0002\n",
            "D_loss:  0.21625184\n",
            "G_loss:  0.49226004\n",
            "reconstruction_loss:  -0.13582188\n",
            "Elapsed time 0:04:32 : ETA in 2:04:49\n",
            "\n",
            "\n",
            "Epoch --------------------- 8 / 200\n",
            "Loop index ---------------- 11 / 40\n",
            "  Generator losses:\n",
            "G_loss [0.520157, -0.08620722, -0.048393145, 0.89100826, 0.9751524]\n",
            "G_loss [0.47573948, -0.086648196, -0.049174394, 0.8587745, 0.97519094]\n",
            "G_loss [0.413373, -0.08641331, -0.049305018, 0.79603213, 0.9745242]\n",
            "  Summary:\n",
            "D_lr: 0.0002\n",
            "G_lr 0.0002\n",
            "D_loss:  0.055351604\n",
            "G_loss:  0.413373\n",
            "reconstruction_loss:  -0.13571833\n",
            "Elapsed time 0:04:41 : ETA in 2:04:38\n",
            "\n",
            "\n",
            "Epoch --------------------- 8 / 200\n",
            "Loop index ---------------- 21 / 40\n",
            "  Generator losses:\n",
            "G_loss [0.4669487, -0.087971136, -0.05457879, 0.91741353, 0.97503436]\n",
            "G_loss [0.42245412, -0.08902448, -0.05582112, 0.89668024, 0.97422993]\n",
            "G_loss [0.37419724, -0.08908025, -0.056887645, 0.86118734, 0.9726888]\n",
            "  Summary:\n",
            "D_lr: 0.0002\n",
            "G_lr 0.0002\n",
            "D_loss:  0.08288367\n",
            "G_loss:  0.37419724\n",
            "reconstruction_loss:  -0.1459679\n",
            "Elapsed time 0:04:51 : ETA in 2:04:21\n",
            "\n",
            "\n",
            "Epoch --------------------- 8 / 200\n",
            "Loop index ---------------- 31 / 40\n",
            "  Generator losses:\n",
            "G_loss [0.4396283, -0.08787637, -0.0557672, 0.9017728, 0.9742912]\n",
            "G_loss [0.397933, -0.088104285, -0.056452584, 0.8695822, 0.9739195]\n",
            "G_loss [0.35687208, -0.08755747, -0.05574184, 0.817886, 0.97197926]\n",
            "  Summary:\n",
            "D_lr: 0.0002\n",
            "G_lr 0.0002\n",
            "D_loss:  0.092465445\n",
            "G_loss:  0.35687208\n",
            "reconstruction_loss:  -0.14329931\n",
            "Elapsed time 0:05:00 : ETA in 2:04:06\n",
            "\n",
            " \n",
            " -------------------------Saving images for epoch 8 ------------------------- \n",
            " \n",
            "\n",
            "\n",
            "\n",
            "Epoch --------------------- 9 / 200\n",
            "Loop index ---------------- 1 / 40\n",
            "  Generator losses:\n",
            "G_loss [0.51525736, -0.087078236, -0.052119333, 0.9317049, 0.9755281]\n",
            "G_loss [0.4939872, -0.086728916, -0.05208038, 0.9080062, 0.974074]\n",
            "G_loss [0.44981986, -0.08688581, -0.05350722, 0.87878335, 0.9749668]\n",
            "  Summary:\n",
            "D_lr: 0.0002\n",
            "G_lr 0.0002\n",
            "D_loss:  0.17075677\n",
            "G_loss:  0.44981986\n",
            "reconstruction_loss:  -0.14039303\n",
            "Elapsed time 0:05:10 : ETA in 2:04:09\n",
            "\n",
            "\n",
            "Epoch --------------------- 9 / 200\n",
            "Loop index ---------------- 11 / 40\n",
            "  Generator losses:\n",
            "G_loss [0.46143717, -0.07914484, -0.057962637, 0.8575052, 0.97500676]\n",
            "G_loss [0.47658908, -0.08175753, -0.055822812, 0.87637746, 0.9760151]\n",
            "G_loss [0.39929605, -0.08742088, -0.057031, 0.8692215, 0.97459334]\n",
            "  Summary:\n",
            "D_lr: 0.0002\n",
            "G_lr 0.0002\n",
            "D_loss:  0.2324855\n",
            "G_loss:  0.39929605\n",
            "reconstruction_loss:  -0.14445189\n",
            "Elapsed time 0:05:20 : ETA in 2:03:59\n",
            "\n",
            "\n",
            "Epoch --------------------- 9 / 200\n",
            "Loop index ---------------- 21 / 40\n",
            "  Generator losses:\n",
            "G_loss [0.41753453, -0.089237906, -0.054786045, 0.8813837, 0.97639024]\n",
            "G_loss [0.36232972, -0.089778855, -0.05534164, 0.83779305, 0.97574157]\n",
            "G_loss [0.2267847, -0.09016027, -0.0575353, 0.7283796, 0.97536075]\n",
            "  Summary:\n",
            "D_lr: 0.0002\n",
            "G_lr 0.0002\n",
            "D_loss:  0.07702786\n",
            "G_loss:  0.2267847\n",
            "reconstruction_loss:  -0.14769557\n",
            "Elapsed time 0:05:30 : ETA in 2:03:44\n",
            "\n",
            "\n",
            "Epoch --------------------- 9 / 200\n",
            "Loop index ---------------- 31 / 40\n",
            "  Generator losses:\n",
            "G_loss [0.49353462, -0.0831664, -0.055997632, 0.9079813, 0.97719365]\n",
            "G_loss [0.5153198, -0.08412215, -0.052166402, 0.9004811, 0.9777242]\n",
            "G_loss [0.45188296, -0.087437674, -0.053013764, 0.8824879, 0.9739095]\n",
            "  Summary:\n",
            "D_lr: 0.0002\n",
            "G_lr 0.0002\n",
            "D_loss:  0.21158558\n",
            "G_loss:  0.45188296\n",
            "reconstruction_loss:  -0.14045143\n",
            "Elapsed time 0:05:39 : ETA in 2:03:30\n",
            "\n",
            " \n",
            " -------------------------Saving images for epoch 9 ------------------------- \n",
            " \n",
            "\n",
            "\n",
            "\n",
            "Epoch --------------------- 10 / 200\n",
            "Loop index ---------------- 1 / 40\n",
            "  Generator losses:\n",
            "G_loss [0.4449811, -0.087355815, -0.05571031, 0.89666784, 0.97897446]\n",
            "G_loss [0.36011255, -0.08905287, -0.056815553, 0.8407467, 0.97805005]\n",
            "G_loss [0.26765764, -0.08886471, -0.057590295, 0.75521755, 0.9769901]\n",
            "  Summary:\n",
            "D_lr: 0.0002\n",
            "G_lr 0.0002\n",
            "D_loss:  0.14087701\n",
            "G_loss:  0.26765764\n",
            "reconstruction_loss:  -0.146455\n",
            "Elapsed time 0:05:49 : ETA in 2:03:31\n",
            "\n",
            "\n",
            "Epoch --------------------- 10 / 200\n",
            "Loop index ---------------- 11 / 40\n",
            "  Generator losses:\n",
            "G_loss [0.36146724, -0.09116435, -0.059244253, 0.8876936, 0.9778597]\n",
            "G_loss [0.30801558, -0.09218314, -0.06077391, 0.86042607, 0.9771601]\n",
            "G_loss [0.25798756, -0.092626095, -0.06234499, 0.83116186, 0.9765366]\n",
            "  Summary:\n",
            "D_lr: 0.0002\n",
            "G_lr 0.0002\n",
            "D_loss:  0.18356301\n",
            "G_loss:  0.25798756\n",
            "reconstruction_loss:  -0.1549711\n",
            "Elapsed time 0:05:59 : ETA in 2:03:20\n",
            "\n",
            "\n",
            "Epoch --------------------- 10 / 200\n",
            "Loop index ---------------- 21 / 40\n",
            "  Generator losses:\n",
            "G_loss [0.33028603, -0.09189136, -0.06215472, 0.892748, 0.97799885]\n",
            "G_loss [0.29646993, -0.09221304, -0.06205926, 0.86212647, 0.9770664]\n",
            "G_loss [0.24392235, -0.09219259, -0.06253274, 0.8139268, 0.9772488]\n",
            "  Summary:\n",
            "D_lr: 0.0002\n",
            "G_lr 0.0002\n",
            "D_loss:  0.14637765\n",
            "G_loss:  0.24392235\n",
            "reconstruction_loss:  -0.15472533\n",
            "Elapsed time 0:06:08 : ETA in 2:03:05\n",
            "\n",
            "\n",
            "Epoch --------------------- 10 / 200\n",
            "Loop index ---------------- 31 / 40\n",
            "  Generator losses:\n",
            "G_loss [0.42154026, -0.08430775, -0.06044575, 0.8917538, 0.9773215]\n",
            "G_loss [0.5291448, -0.084046714, -0.05156464, 0.90477294, 0.98048544]\n",
            "G_loss [0.3843578, -0.08977426, -0.05868321, 0.88911164, 0.97982085]\n",
            "  Summary:\n",
            "D_lr: 0.0002\n",
            "G_lr 0.0002\n",
            "D_loss:  0.1702192\n",
            "G_loss:  0.3843578\n",
            "reconstruction_loss:  -0.14845747\n",
            "Elapsed time 0:06:18 : ETA in 2:02:51\n",
            "\n",
            " \n",
            " -------------------------Saving images for epoch 10 ------------------------- \n",
            " \n",
            "\n",
            "\n",
            "\n",
            "Epoch --------------------- 11 / 200\n",
            "Loop index ---------------- 1 / 40\n",
            "  Generator losses:\n",
            "G_loss [0.35432267, -0.09071693, -0.058476888, 0.867164, 0.9790968]\n",
            "G_loss [0.28667402, -0.092279516, -0.060483266, 0.835334, 0.97896785]\n",
            "G_loss [0.19559681, -0.0916942, -0.062138446, 0.7556305, 0.9782928]\n",
            "  Summary:\n",
            "D_lr: 0.0002\n",
            "G_lr 0.0002\n",
            "D_loss:  0.103623554\n",
            "G_loss:  0.19559681\n",
            "reconstruction_loss:  -0.15383264\n",
            "Elapsed time 0:06:28 : ETA in 2:02:50\n",
            "\n",
            "\n",
            "Epoch --------------------- 11 / 200\n",
            "Loop index ---------------- 11 / 40\n",
            "  Generator losses:\n",
            "G_loss [0.30030257, -0.091611244, -0.0661063, 0.8986746, 0.97880334]\n",
            "G_loss [0.27495605, -0.09240629, -0.067193635, 0.89157313, 0.97938216]\n",
            "G_loss [0.21833104, -0.09411538, -0.068390526, 0.8658837, 0.97750634]\n",
            "  Summary:\n",
            "D_lr: 0.0002\n",
            "G_lr 0.0002\n",
            "D_loss:  0.1391544\n",
            "G_loss:  0.21833104\n",
            "reconstruction_loss:  -0.1625059\n",
            "Elapsed time 0:06:38 : ETA in 2:02:39\n",
            "\n",
            "\n",
            "Epoch --------------------- 11 / 200\n",
            "Loop index ---------------- 21 / 40\n",
            "  Generator losses:\n",
            "G_loss [0.30211878, -0.091734864, -0.063163444, 0.8716959, 0.979406]\n",
            "G_loss [0.24948192, -0.09189801, -0.06404274, 0.8293835, 0.9795059]\n",
            "G_loss [0.17691058, -0.091565266, -0.06423157, 0.7557155, 0.97916347]\n",
            "  Summary:\n",
            "D_lr: 0.0002\n",
            "G_lr 0.0002\n",
            "D_loss:  0.11626269\n",
            "G_loss:  0.17691058\n",
            "reconstruction_loss:  -0.15579683\n",
            "Elapsed time 0:06:47 : ETA in 2:02:25\n",
            "\n",
            "\n",
            "Epoch --------------------- 11 / 200\n",
            "Loop index ---------------- 31 / 40\n",
            "  Generator losses:\n",
            "G_loss [0.25818723, -0.0907198, -0.06789417, 0.86500245, 0.97932446]\n",
            "G_loss [0.19861013, -0.09197805, -0.068125546, 0.82035196, 0.9792941]\n",
            "G_loss [0.1350829, -0.09000327, -0.06868829, 0.7437495, 0.9782489]\n",
            "  Summary:\n",
            "D_lr: 0.0002\n",
            "G_lr 0.0002\n",
            "D_loss:  0.2137004\n",
            "G_loss:  0.1350829\n",
            "reconstruction_loss:  -0.15869156\n",
            "Elapsed time 0:06:56 : ETA in 2:02:11\n",
            "\n",
            " \n",
            " -------------------------Saving images for epoch 11 ------------------------- \n",
            " \n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-4ccfc17d9115>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-34-3f87dc97d349>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, opt)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# Train on image batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0mrun_training_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;31m# Start timer after first (slow) iteration has finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-3f87dc97d349>\u001b[0m in \u001b[0;36mrun_training_batch\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'generator_iterations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             G_loss.append(model['G_model'].train_on_batch(\n\u001b[0;32m---> 30\u001b[0;31m                 x=[real_images_A, real_images_B], y=target_data))\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# =====================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2483\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2485\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2486\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 968\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    969\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1191\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1192\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1369\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1371\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1372\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1375\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1377\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1378\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1359\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1361\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1453\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1454\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1455\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1457\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}
